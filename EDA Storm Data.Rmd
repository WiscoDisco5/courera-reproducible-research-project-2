---
title: 'Reproducible Research: Course Project 2'
author: "John Goodwin"
date: "May 14, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Processing

Start by loading required packages.

```{r}
library(tidyverse)
library(lubridate)

```

Then download and read in the storm data.


```{r}
if(!file.exists("StormData.csv.bz2")) {
  download.file("https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2", destfile = "StormData.csv.bz2")
}

storm <- read_csv("StormData.csv.bz2")
```
Let's clean up the the date and time fields and place them in one field.

```{r}
storm <- storm %>% 
  mutate(BGN_TIME = gsub('^([0-9]{2})([0-9]+)$', '\\1:\\2', BGN_TIME),
         BGN_DATE = gsub(" .*", "",BGN_DATE),
         BGN_DATETIME = paste(BGN_DATE, BGN_TIME),
         BGN_DATETIME1 = parse_date_time(BGN_DATETIME, "%m/%d/%Y %H:%M:%S %p"),
         BGN_DATETIME2 = parse_date_time(BGN_DATETIME, "%m/%d/%Y %H:%M"),
         BGN_DATETIME = coalesce(BGN_DATETIME1, BGN_DATETIME2)) %>%
  select(-BGN_DATETIME1, -BGN_DATETIME2)
  
```

It looks like some dates/times aren't parsing properly.

```{r}
storm %>% filter(is.na(BGN_DATETIME)) %>% 
  select(BGN_DATE, BGN_TIME, BGN_DATETIME)
```

Looks like some times weren't entered properly. To avoid making edits to individual rows, these will just be left as is.  

Now let's clean up the damage costs fields. It looks like the true severity of an event is contained in two columns where field gives the magnitude of the damage (thousands, millions, and billions) and the other gives 3 significant digits. This code combines this information into one field.

```{r}
storm <- storm %>% 
  mutate(PROPDMG_factor = recode(PROPDMGEXP,
                                 K = 1000, 
                                 M = 1000000, 
                                 B = 1000000000, 
                                 .default = 0, 
                                 .missing = 0),
         CROPDMG_facotr = recode(CROPDMGEXP,
                                 K = 1000, 
                                 M = 1000000, 
                                 B = 1000000000, 
                                 .default = 0, 
                                 .missing = 0),
         PROPDMG = PROPDMG * PROPDMG_factor,
         CROPDMG = CROPDMG * CROPDMG_facotr) %>%
  select(-CROPDMG_facotr, -PROPDMG_factor)
```

Finally, it looks like we are dealing with some data that is spread out over a long period of time. It might be a good idea to add an adjustment for inflation. That way old events do not appear less severe just because they're old. The blscrapeR package will give us an inflation factor to apply by year.

```{r}
library(blscrapeR)

inflation <- inflation_adjust(2018) %>% select(year, adj_value) %>% mutate(year = as.numeric(year))

storm <- storm %>% 
  mutate(year = year(BGN_DATETIME)) %>% 
  left_join(inflation, by = "year")  %>%
  mutate(PROPDMG_adj = PROPDMG / adj_value,
         CROPDMG_adj = CROPDMG / adj_value)


storm %>% group_by(year) %>% summarise(prop = mean(PROPDMG),
                                       prop_adj = mean(PROPDMG_adj)) %>%
  gather(adj, sev, -year) %>%
  ggplot(aes(year, sev, color=adj, group = adj)) +
  geom_line()


```


##Public Health and Weather

What types of weather events have harmful impacts on population health? To begin to answer this questions we have to do a better job of sorting events. Look at the top ten most frequent events:

```{r}
storm %>% group_by(EVTYPE) %>% 
  summarise(Frequency = n()/nrow(.)) %>% 
  arrange(desc(Frequency)) %>% top_n(20) %>%
  knitr::kable(.,format = "html")
```

Note the similarities between some of the fields (hign wind vs. strong wind, etc.). I am going to try to reduce some of these redundancies using techniques used for text mining ([I learned about most of this from here!](https://www.tidytextmining.com/index.html)). Basically, I am going to split up character strings like "HIGH WIND" to "HIGH" and "WIND". After that I am going to

```{r}
library(tidytext)
library(stringr)
library(SnowballC)

#, PROPDMG, CROPDMG, INJURIES, FATALITIES

tokens <- storm %>% select(REFNUM, EVTYPE) %>%
  unnest_tokens(event_words, EVTYPE) %>%
  mutate(event_words = wordStem(event_words),
         event_words = str_replace_all(event_words, "[:punct:]", " ")) %>%
  unique # this removes any words that appear twice in a EVTYPE

```
And what are the most frequent words now?

```{r}
obs <- nrow(storm)

tokens %>% group_by(event_words) %>% summarise(Frequency =n()/obs) %>%
  arrange(desc(Frequency)) 
```


```{r}



%>%
  group_by(event_words) %>%                         
  mutate(count = n()) %>% ##doing what i expect?
  filter(count > 50) %>% 
  ungroup

##cluster
tokens_summarised <- tokens %>% group_by(event_words) %>% select(-REFNUM, -count) %>%
  summarise_all(mean) %>% 
  as.data.frame

names <- tokens_summarised$event_words
tokens_summarised$event_words <- NULL

logger <- function(x) {if_else(x > 0, log(x), 0)}
tokens_summarised <- apply(tokens_summarised,2, logger)
row.names(tokens_summarised) <- names


tokens_cluster <- tokens_summarised %>% dist %>% hclust
plot(tokens_cluster)

x<-cutree(tokens_cluster,8)

severity_by_cluster <- function(k) {
  x <- cutree(tokens_cluster, k) 
  word_to_clust <- bind_cols(event_words = names(x), cluster = x)
  
  raw_results <- tokens %>% 
    select(REFNUM, event_words, PROPDMG, CROPDMG, INJURIES, FATALITIES) %>% 
    left_join(word_to_clust, by = "event_words")  %>%
    group_by(cluster) %>%
    select(cluster,PROPDMG, CROPDMG, INJURIES, FATALITIES) %>%
    summarise_all(mean)
  
  list(word_to_clust, raw_results)

}

clusters <- severity_by_cluster(8)
top_prop <- clusters[[2]] %>% arrange(desc(PROPDMG)) %>% .[1,]
clusters[[1]] %>% filter(cluster == top_prop$cluster)

```




--cluster econ
--cluster pop health
--add anyhting else to these--lat long?

--mapping for concentrated losses?






